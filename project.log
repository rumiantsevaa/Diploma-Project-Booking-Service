There will be listed Documentation of the Current Project Progress. 

09.12
A GitHub repository has been created: https://github.com/rumiantsevaa/Diploma-Project-Booking-Service.
Access has been configured for the team. The designer has been added to the project as a collaborator and can work in their own branch: /design.
A domain name for the website has been rented on the nic.ua portal bbooking.pp.ua. The domain is valid for one year.

10.12
Updated the domain’s Nameservers (NS) to Cloudflare’s to manage the domain through Cloudflare.
Added bbooking.pp.ua to Cloudflare, enabling performance, security, and encryption features.
Updated DNS records, including: A CNAME record for www.bbooking.pp.ua, pointing to rumiantsevaa.github.io, and set proxy status to Proxied for security.

Initially, the custom domain in GitHub Pages was set to bbooking.pp.ua. 
This setup led to DNS resolution issues due to GitHub's restrictions on using CNAME for the root domain (bbooking.pp.ua).
The solution was to change the custom domain to www.bbooking.pp.ua in GitHub Pages, as GitHub Pages can support a CNAME record for subdomains like www. 
This fixed the DNS issue and allowed proper routing of the custom domain.

The website is now accessible at https://www.bbooking.pp.ua/.
Cloudflare’s security and performance features are active, including HTTPS encryption.
GitHub Pages automatically builds and deploys the site whenever changes are made to the main branch on GitHub in the /docs folder, ensuring the site is always up to date with the latest changes.

11.12 
The website migration to AWS server.

12.12 
Nginx set up on AWS server.
SSL Certificates for both www.bbooking.pp.ua/ bbooking.pp.ua issued.

18.12
Manual NGINX REVERSE PROXY set up confugured in container with SSL Certification on AWS (volumes for copying added), Docker Compose manual build and deployment done. 
The general traffic flow will look like the following: Client -> NGINX -> WSGI -> Flask. Everything but certificates managed and set up inside the docker with external access via port forwarding. 
Proceed automation + Repository file structure migration. 

MAJOR PROJECT LOG UPDATE:

Current Status:
The website is fully operational and accessible at both https://bbooking.pp.ua and https://www.bbooking.pp.ua.
SSL enforcement is configured, ensuring all traffic is securely redirected to https:// with no access allowed via http://.
Both www and non-www segments are functional and properly routed to the application.

Deployment Process:
Deployment is now fully automated and tested. Changes pushed to the main branch on GitHub trigger the following steps:
*Connect to the AWS server via SSH.
*Stop any currently running Docker containers (without deletion).
*Sync repository files to the server.
*Rebuild services with docker compose build.
*Restart the application with docker compose up -d.

Changes to Security Configuration:
Cloudflare integration has been removed, including its security, performance, and DNS management features.
This decision was made to transition to a self-hosted protection mechanism, allowing greater flexibility and control over traffic and security measures.

Summary of Key Features:
*Fully automated deployment pipeline.
*Secure, SSL-enforced access to both www.bbooking.pp.ua and non-www bbooking.pp.ua domains (Redirects 80 => 443).
*Auto NGINX reverse proxy configuration within a Docker container for external SSL management.
*All critical components, except certificates, are handled inside Docker with external access enabled via port forwarding.

19.12
Certbot renewal is set up via crontab.

23.12
Deployment trigger:
* Fired when pushing to the main branch
* Ignores changes in the version.txt file
Main steps:
* Checkout code from the repository
* Login to Docker Hub using secret credentials
Versioning:
* Generates a new Docker tag based on semantic versioning
* Increments the patch version by 1
* Updates the version.txt file
* Creates a new git tag
* Pushes changes back to the repository
* Building and publishing Docker images:
* Builds a Flask application and pushes to Docker Hub
* Builds an Nginx image and pushes to Docker Hub
* Uses the new version for image tags
Deploy to AWS:
* Connects to an AWS server via SSH
* Creates/updates the project directory
* Clones/updates the code from the repository
* Gets the current version from version.txt
* Creates a .env file with current version
* Stops current containers
* Cleans unused images
* Pull new images
* Restart containers via docker compose
The entire process is automated and runs on every push to the main branch. This ensures continuous delivery (CD) of the application to the production server.

Added Personal Access Token to the Update version files block.
The token is used for authorization when pushing changes to a new version.txt file containing the version tag back to the repository: git push ...//${{ secrets.PAT }}.
----------------------------------------------------------------------------------------------------------------
! Why use PAT and not GITHUB_TOKEN?
GITHUB_TOKEN is automatically generated for each GitHub Actions run, but: * Limited to tasks inside Actions only.

PAT allows you to:
- Push changes and tags.
- Work with other repositories.
- Set access levels (e.g. read, write).
----------------------------------------------------------------------------------------------------------------
! Fixed infinite version update loop in GitHub Actions workflow by adding paths-ignore for version.txt in deploy.yml.
This improvement ensures that:
- Version updates only trigger on actual code changes
- Prevents recursive workflow triggers from version file updates
- Maintains clean versioning history
- Deployment automation continues to work as expected with version increments only on meaningful changes

The deployment process remains fully automated with the following workflow:
1. Code changes pushed to main branch trigger the workflow
2. New version tag is generated and version.txt is updated
3. Docker images are built and pushed with new version tags
4. AWS server deploys updated containers

--- DOCKER HUB LIMITATIONS OVERCOME --- 

Docker Hub does not support uploading a complete compose.yml file as a single entity. 

The project uses docker-compose.yaml to orchestrate two main services:
* nginx-proxy (Frontend proxy server)
* flask-app (Backend application)

The solution is to store compose.yml file in a version-controlled repository like GitHub and reference the individual images hosted on Docker Hub within that file.

Docker Hub and GitHub Integration:
! The project uses both Docker Hub and GitHub because:

* Docker images are stored on Docker Hub with versioning (using ${TAG} variable)
* Configuration files and application code need to be mounted/copied from GitHub:
  - /nginx/default.conf (Nginx configuration)
  - SSL certificates (/etc/letsencrypt)
  - Application code and requirements

Key files cloned from GitHub:
* Flask application: app.py (main application) - requirements.txt (dependencies)

* Nginx configuration: default.conf (server configuration) - nginx.conf (main configuration) - start.sh (initialization script)

Communication Flow: Client Request -> Nginx (443/80) -> Flask App (8000)

! Security Features:
- SSL/TLS encryption
- Automatic HTTP to HTTPS redirection
- Non-root users in containers
- Proper file permissions
- Health checks for both services

Deployment Process:
* Images are built and tagged with versions
* Both images are pushed to Docker Hub (serpentariya/booking_nginx and serpentariya/booking_app)
* During deployment, docker-compose pulls images from the Docker Hub and mounts necessary files from the cloned GitHub repository

This architecture allows for:
* Version control of both code and Docker images
* Secure SSL termination
* Proper separation of concerns between frontend proxy and application server
* Easy deployment and scaling
* Efficient resource utilization
_______________________________________________________________________________________________________
The split between Docker Hub (for images) and GitHub (for configuration and code) is necessary because:
_______________________________________________________________________________________________________
1. Docker Hub is optimized for storing and distributing container images
2. GitHub provides version control for configuration and code files
3. Some files (like SSL certificates) need to be mounted at runtime and shouldn't be baked into images
4. This separation allows for better security and maintenance practices

24.12 
deploy.yml changes: Deploy to AWS Job //
Removed git clone/pull to only copy the necessary files via scp due to the project structure.
The directory structure simplified - create only the necessary ones.
.
├── compose.yaml
└── nginx
    └── default.conf
2 directories, 2 files (needed for compose.yaml ( WHICH IS A CRUCIAL HANDLER to operate and maintain pulled from Docker Hub images that were built and uploaded in previous steps as requested.))

* Operations Left:
- Copying configuration files: compose.yaml and default.conf
- Creating .env with version
- Stopping old containers
- Cleaning unused images
- Loading new images
- Starting new containers

NOW I CAN OFFICIALLY ANNOUNCE : IT WORKS! WORKS! OPTIMIZED! OPTIMIZED!  
Current version is v1.0.27

25.12 
Docker and Docker Hub check and installation on AWS Server added.
Certificate paths optimized - now using a single SSL certificate for both domains (bbooking.pp.ua and www.bbooking.pp.ua). 
Previously, no-www domain was using a separate certificate, but now both domains are covered by one certificate, which simplifies management and improves efficiency. 
This is reflected in nginx/default.conf where both domains use the same certificate path:
ssl_certificate /etc/letsencrypt/live/www.bbooking.pp.ua/fullchain.pem
ssl_certificate_key /etc/letsencrypt/live/www.bbooking.pp.ua/privkey.pem 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
*** SSL Certificate Management Automation Added to deploy.yml : *** 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
- Automated Certbot installation check and setup
- Automated crontab management (installation, enabling, and starting of cron service)
- Automated certificate renewal task configuration:
  * Sets up automatic renewal at 3 AM on the first day of each month
  * Replaces previous manual crontab setup from December 19th
- Implemented intelligent certificate handling:
  * Checks for existing certificate for both domains (bbooking.pp.ua and www.bbooking.pp.ua)
  * Automatically renews existing certificate when needed
  * Obtains new certificate if none exist(in case of server migration or project files corruption)
  * Manages Docker containers during certificate operations (stops before renewal, starts after)
  * Uses non-interactive mode with automated email configuration

♡ This automation eliminates the need for manual SSL certificate management and ensures consistent certificate handling across all deployments. 
♡ The system now automatically maintains SSL security without human intervention.

30.12
CloudFlare integrated again as a possible future part of security and monitoring measures due to potential lack of OM on AWS. 
Cloudflare log format Added to nginx.conf.
Cloudflare IP ranges Added to default.conf
FIX 301 for /health-check to bypass CloudFlare in default.conf due to CloudFlare's HTTPS redirect enforcement.
Register at deploy.yml is more clear now. Less output to read. 

Security Analysis Integration:
Enhanced security measures implemented through two powerful scanning tools:
1. Trivy Scanner:
- Integrated for internal vulnerability scanning
- Analyzes Docker containers and application dependencies
- Detects vulnerabilities in code and dependencies
- Currently operational and providing security insights
- Configured to check CRITICAL, HIGH, and MEDIUM severity issues
- Set to continue deployment even if vulnerabilities are found (non-blocking)

2. OWASP ZAP (Zed Attack Proxy):
- Implemented for external security analysis
- Performs automated security testing of the live application
- Scans for web application vulnerabilities
- Currently experiencing some technical issues
- Resolution in progress and expected soon
- Will provide comprehensive external security assessment when fully operational
Note: These security implementations do not affect the website's overall functionality and performance. 
The application remains fully operational while these security measures are being fine-tuned. 

!!! Trivy scans are successfully running in the deployment pipeline, while ZAP integration issues are being addressed.



31.12
ZAP is working properly, saving reports to the deploy output on GitHub actions. Always accessible and no memory on AWS needed. Happy New Year!  

02.01
Added segment for restriction of direct access to the website via IP. 

03.01 
``````````````````````
~ Netdata Monitoring~ 
``````````````````````
Features of Netdata that were implemented in the project: 
- Real-time performance monitoring system
- Designed to collect and visualize system and application performance metrics
- Has a built-in web interface for displaying data
- Main configuration components (/netdata/netdata.conf):
- Hostname is set to bbooking.pp.ua
- Binding to local IP (127.0.0.1)
- Web interface runs on port 19999
- Health and registry modules are enabled
- Plugins for monitoring processes (proc), disk space (diskspace) and cgroups are activated

Automation and management:
- Script setup_netdata.sh for automatic installation and configuration
- save_dashboard.sh for periodic saving of dashboards was removed due to inefficiency 

Collected metrics: CPU usage / Memory consumption / Disk / I/O (Input/Output) operations / Network traffic / Process monitoring / Disk space usage

Security:

! Access is limited only local host (127.0.0.1)
! Web interface is only available locally
! Static web server mode

Advantages of choosing Netdata:
- Real-time monitoring
- Low overhead
- A rich set of built-in metrics
- Convenient data visualization

CI/CD integration:

`` Built-in installation in the deployment process
`` Automatic recovery during migration
`` Saving the configuration in the repository
`` Quick deployment when needed

Cloudflare as an additional monitoring layer:
`` Availability monitoring
`` Traffic analytics
`` Performance metrics
`` Request logging (a special log format is configured in nginx.conf)

Built-in health checks:
`` Health checks in Nginx (/health-check endpoints)
`` SSL certificate monitoring
`` Service availability checks

! Optimal for a small EC2 instance:
- Netdata is optimized for small resources
- Local storage of only the necessary metrics
- Load distribution between different tools
- Efficient use of existing services (Cloudflare)

Additional benefits: Automation of all processes / Minimal manual intervention / Quick recovery from failures  / Comprehensive approach to monitoring 

This configuration fully meets the monitoring requirements, while effectively using the resources of a small EC2 instance.

Additional: It is planned to add Netdata screenshots with examples of metrics collection. Possibility of using variety of plugins. 
Possibility of customization via configuration files. 

04.01
Netdata screenshots added.
MINOR FIX: Update version to v1.0.132 -> NOW Trivy SCANS THE SAME BUILD AND NOT PREVIOUS -> Running Trivy with options: trivy image ***/booking_app:v1.0.132. 

05.01
--- VERSION MANAGEMENT ENHANCEMENT ---

Implemented a flexible versioning system by introducing a 'ver_type' variable in the deployment workflow. This enhancement allows for more controlled version increments based on the type of changes being deployed.
The versioning system now supports three modes:
1. VER_TYPE=1 (Default - Patch):
   - Increments the last number (patch version)
   - Example: v1.0.1 -> v1.0.2
   - Used for bug fixes and minor changes
2. VER_TYPE=2 (Minor):
   - Increments the middle number and resets patch to 0
   - Example: v1.0.1 -> v1.1.0
   - Used for new features that maintain backwards compatibility
3. VER_TYPE=3 (Major):
   - Increments the first number and resets others to 0
   - Example: v1.0.1 -> v2.0.0
   - Used for breaking changes or significant updates
Benefits:
- More granular control over version numbers
- Better reflection of the significance of changes
- Follows semantic versioning principles
- Maintains automated deployment workflow
- Provides flexibility in version management

This improvement allows the team to better communicate the nature and impact of changes through version numbers while maintaining the automated deployment process.
Test is successful! v1.0.145 ->  v1.1.0 - (Minor)  Test for Major transition is not due yet.  v.2.0.0 will be very soon! Stay tuned. 

--- Deployment Rerun Enhancement ---
Added logic to check if a deployment is a rerun by verifying the existence of git tags
If it's a rerun, the version.txt file remains unchanged to prevent conflicts
This fixes the previous error: fatal: tag 'vX.X.X' already exists

Added an alert system that notifies users when they're performing a rerun
Automatically handles existing Docker images by removing them before recreation
The logic is implemented in two main steps: Version Check Step | Docker Image Check Step

These improvements ensure that:
* Deployments can be safely rerun without version conflicts
* Users are properly notified of rerun operations
* Docker images are handled appropriately during reruns
* The version control system maintains its integrity
* This enhancement significantly improves the deployment workflow reliability and user experience when reruns are necessary.


07.01
--- DEPLOYMENT WORKFLOW ENHANCEMENT: ERROR HANDLING & ROLLBACK MECHANISM ---

The deployment workflow has been significantly enhanced with robust error handling and automatic rollback capabilities:

1. Last Successful Version Tracking:
- Implemented storage of the last known working version in last_successful_version.txt
- This version serves as a fallback point for recovery
- File is automatically updated only after successful deployments

2. Enhanced Deployment Process:
- Added / continue-on-error: true / to allow controlled error handling
- Deployment step now includes status checking
- Automatic detection of any deployment failures
- Integrated rollback mechanism for recovery

3. Rollback Mechanism:
When deployment fails (namely obtaining exit code x errors):
- System automatically initiates rollback procedure
- Pulls previous working version from Docker Hub
- Updates environment with last successful version
- Restarts containers with proven stable version
- Exits workflow after successful rollback

4. Version Management:
Successful deployments:
- Update last_successful_version.txt with new version
- Commit and push changes to repository
- Maintain version history for tracking

Failed deployments:
- Preserve previous successful version
- No updates to version tracking files
- Provide clear failure notifications

5. Workflow Control:
- Added explicit workflow exit after rollback
- Prevents further steps execution after failure
- Maintains system stability during issues

Benefits:
- Zero downtime deployment strategy
- Automatic recovery from failures
- Maintained system stability
- Clear deployment status tracking
- Reliable version history

This enhancement significantly improves system reliability by ensuring that any deployment issues are automatically handled with a fallback to the last known working state, 
minimizing potential downtime and maintaining service availability.
Therefore, my project deployment has evolved from a basic CI/CD setup into a proper deployment pipeline with production-ready features and safety mechanisms.

--- ROLLBACK MECHANISM OPTIMIZATION ---
Enhanced rollback functionality by introducing a unified rollback_deployment() function that is used in two critical scenarios: 
1.Deployment Failure Rollback(already set up in described below: viz. DEPLOYMENT WORKFLOW ENHANCEMENT: ERROR HANDLING & ROLLBACK MECHANISM) 
2.Health Check Failure Rollback (NEW) -> Triggers when containers fail health checks & Monitors both Nginx and Flask container health status

Rollback Function Implementation benefits : / Reduced code duplication / Consistent rollback behavior
While adding Health Check Implementation :  / Validates service availability before completing deployment  / Prevents deployment of non-functional containers  / Ensures both Nginx and Flask services are fully operational

! Critical improvement to prevent premature health checks -> Initialization Pause Addition:
* Added 15-second pause after container startup that prevents false-negative health check results

This enhancement ensures that containers are given adequate time to initialize and properly start their services before being subjected to health checks, significantly improving the reliability 
of my deployment process and reducing unnecessary rollbacks due to premature health checks.

10.01 
DOCKER_HUB_NICKNAME GitHub secret added.
